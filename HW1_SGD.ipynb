{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to code an optimization algorithm that optimzes the penalized loss function of the logistic regression model.\n",
    "\n",
    "You have to send the filled notebook named **\"L2_familyname1_familyname2.ipynb\"** by email to *violeta.roizman@l2s.centralesupelec.fr* by October 10, 2018. Please put **\"AML-L2\"** in the subject. \n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the W8A dataset, a tidy and binarized version of https://archive.ics.uci.edu/ml/datasets/adult (check it out for more details). \n",
    "In this dataset we have census data to predict if the income of an adult exceeds $50K/yr (1 or -1).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w8a_train = pd.read_csv(\"w8a.csv\", sep=\";\")\n",
    "w8a_train_x = w8a_train.iloc[:, :-1].values\n",
    "w8a_train_y = w8a_train.iloc[:, -1].values\n",
    "\n",
    "w8a_test  = pd.read_csv(\"w8a_t.csv\", sep=\";\")\n",
    "w8a_test_x = w8a_test.iloc[:, :-1].values\n",
    "w8a_test_y = w8a_test.iloc[:, -1].values\n",
    "\n",
    "print(\"shape: \", w8a_train.shape)\n",
    "# w8a_train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we’ll be moving from linear regression to logistic regression, one of the simplest ways to deal with a classification problem. Instead of fitting a line, logistic regression models the probability that the outcome is 1 given the value of the predictor. In order to do this we need a function that transforms our predictor variable to a value between 0 and 1. Lots of functions can do that, but the logistic function is the most common choice:\n",
    "\n",
    "$$f(z) = \\frac{1}{1+\\exp{-z}}.$$\n",
    "\n",
    "To predict the class of our observations we'll have to minimize the corresponding loss function and as we are in a high-dimensional context we'll add an $l_2$ regularization to the model:\n",
    "\n",
    "$$L(\\textbf{w}) = \\sum_{i=1}^n log(1+\\exp(-y_i\\textbf{w}^Tx_i))+\\frac{\\lambda}{2} \\| \\textbf{w} \\|^2,$$\n",
    "\n",
    "where $x_i$ is the vector of features for the observation $i$ and $y_i \\in \\{-1, 1\\}$ is the class label.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the `sklearn` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty=\"l2\", C=1.0)\n",
    "model.fit(w8a_train_x, w8a_train_y)\n",
    "y_pred = model.predict(w8a_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we compute the accuracy score to evaluate the model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(w8a_test_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Implement from scratch your own logistic regression model with stochastic gradient descent. \n",
    "\n",
    "- Fill in the classes\n",
    "\n",
    "- Display the evolution of the cost function along iterations. Do this for several strategies for the setting of the learning rate\n",
    "\n",
    "- Try the different acceleration strategies\n",
    "\n",
    "- Train the model with the training set and evaluate its performance in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\" Class for logistic regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients \n",
    "    alpha_: float\n",
    "        regularization parameter\n",
    "    bsize: integer\n",
    "        size of the mini-batch\n",
    "    averaging: boolean\n",
    "        if true averaging is used, if not it isn't\n",
    "    acceleration: string\n",
    "        name of the aceleration strategy\n",
    "    lr: float\n",
    "        the learning rate\n",
    "    max_iter:\n",
    "        maximum number of iterations of the optimization algorithm\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, bsize, averaging, acceleration, lr, max_iter):\n",
    "        self.coef_  = None\n",
    "        self.alpha_ = alpha\n",
    "        self.bsize  = bsize\n",
    "        self.averaging = averaging\n",
    "        self.acceleration  = acceleration\n",
    "        self.lr     = lr\n",
    "        self.max_iter = max_iter\n",
    "        #Loss function values\n",
    "        self.l_f = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "        def f_lr(beta):            \n",
    "            \"\"\" \n",
    "            Returns the logistic loss.        \n",
    "            \"\"\"\n",
    "            \n",
    "            logistic_loss = 0\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                \n",
    "                tmp = -y[i]*beta.T.dot(X[i, :])\n",
    "                \n",
    "                if(tmp > 10):\n",
    "                    logistic_loss += + tmp\n",
    "                else:\n",
    "                    logistic_loss += np.log(1 + np.exp(tmp))\n",
    "                                \n",
    "            logistic_loss += self.alpha_*np.linalg.norm(beta)**2\n",
    "                        \n",
    "            return logistic_loss\n",
    "        \n",
    "        def fprime_lr(beta):   \n",
    "            \"\"\" Returns the gradient of f_ls at beta.\n",
    "            \"\"\"\n",
    "            \n",
    "            # grad(F) = L.T * eˆ(L*beta) / (1 + eˆ(L*beta))\n",
    "            \n",
    "            # gradient descent\n",
    "            if(self.bsize == len(y)):\n",
    "                L = -np.diag(y).dot(X)\n",
    "                tmp = L.dot(beta)\n",
    "                grad = L.T.dot(np.exp(tmp)/(1 + np.exp(tmp))) + self.alpha_*beta\n",
    "            \n",
    "            # stochastic gradient descent (mini-batch)\n",
    "            elif(self.bsize > 1 and self.bsize < len(y)):\n",
    "                i_t = randint(0, np.rint((len(y)-1)/self.bsize))\n",
    "                vec = range(self.bsize*(i_t-1)+1, self.bsize*i_t+1)\n",
    "                L = np.zeros((self.bsize, X.shape[1]))\n",
    "                t = 0\n",
    "                for i in vec:\n",
    "                    L[t, :] = -y[i]*X[i, :]\n",
    "                    t += 1\n",
    "                tmp = np.array(L.dot(beta))\n",
    "                grad = L.T.dot(np.exp(tmp)/(1 + np.exp(tmp))) + self.alpha_*beta*self.bsize/X.shape[0]\n",
    "            \n",
    "            # stochastic gradient descent\n",
    "            elif(self.bsize == 1):\n",
    "                i_t = randint(0, len(y)-1)\n",
    "                L = np.array(-y[i_t]*X[i_t, :])\n",
    "                tmp = L.dot(beta)\n",
    "                grad = L.T*np.exp(tmp)/(1 + np.exp(tmp)) + self.alpha_*beta/X.shape[0]   \n",
    "            \n",
    "            return grad\n",
    "        \n",
    "        \n",
    "        def stochastic_gradient_descent():\n",
    "            \n",
    "            coef = np.zeros((X.shape[1])) \n",
    "                        \n",
    "            loss_value = np.zeros((self.max_iter+1))\n",
    "                        \n",
    "            loss_value[0] = f_lr(coef)\n",
    "            \n",
    "            # vector that contains betaˆ(t-1) for the momentum variant\n",
    "            tmp = np.zeros((X.shape[1]))\n",
    "            \n",
    "            if(self.averaging):\n",
    "                mean = np.zeros((X.shape[1])) \n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                \n",
    "                #print(i)\n",
    "                \n",
    "                if(self.averaging):\n",
    "                    self.lr = np.sqrt(1/(i+1)) \n",
    "                else:\n",
    "                    self.lr = 1/(i+1) \n",
    "                \n",
    "                # parameter for the momentum variant \n",
    "                theta = 0.9\n",
    "                \n",
    "                if(self.acceleration == 'momentum'):\n",
    "                    # theta_t * (betaˆ(t) - betaˆ(t-1)) \n",
    "                    tmp1 = theta*(coef - tmp)\n",
    "                    tmp = np.array(coef) \n",
    "                    # beta^(t+1) = betaˆ(t) - gamma_t * grad(F) + theta_t * (betaˆ(t) - betaˆ(t-1))\n",
    "                    coef -= (self.lr*fprime_lr(coef) - tmp1)\n",
    "                else:\n",
    "                    # beta^(t+1) = betaˆ(t) - gamma_t * grad(F)\n",
    "                    coef -= self.lr*fprime_lr(coef)\n",
    "                    \n",
    "                if(self.averaging):\n",
    "                    # beta_ˆ(t) = (1 - 1/t) * beta_ˆ(t-1) + 1/t * betaˆ(t) \n",
    "                    mean = (1 - 1/(i+1))*mean + (1/(i+1))*coef\n",
    "                    loss_value[i+1] = f_lr(mean)\n",
    "                else:    \n",
    "                    loss_value[i+1] = f_lr(coef)\n",
    "                \n",
    "                #print(loss_value[i+1])\n",
    "                                                    \n",
    "            if(self.averaging):\n",
    "                self.coef_ = mean\n",
    "            else:\n",
    "                self.coef_ = coef\n",
    "                                    \n",
    "            plt.plot(range(i+1), loss_value[range(i+1)])\n",
    "            self.l_f = loss_value\n",
    "        stochastic_gradient_descent()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make binary predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "        temp_pred = np.zeros(X.shape[0]) \n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            temp_pred[i] = np.sign(X[i, :].dot(self.coef_))\n",
    "            #Processing the zero values\n",
    "            if (temp_pred[i] == 0):\n",
    "                temp_pred[i] = np.random.choice([-1,1])\n",
    "        \n",
    "        return temp_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_it = 30\n",
    "n_feat = w8a_train_x.shape[1]\n",
    "lam = 20\n",
    "\n",
    "\"stochastic gradient descent\"\n",
    "dim_batch = 1\n",
    "m1 = LogisticRegression(lam, dim_batch, False, 'l', 1e-2, num_it)\n",
    "m1.fit(w8a_train_x, w8a_train_y)\n",
    "p1 = m1.predict(w8a_test_x)\n",
    "print(\"Accuracy:\" + \" \"+str(accuracy_score(w8a_test_y, p1)))\n",
    "\n",
    "\"stochastic gradient descent - avec avaraging\"\n",
    "m2 = LogisticRegression(lam, dim_batch, True, 'l', 1e-2, num_it)\n",
    "m2.fit(w8a_train_x, w8a_train_y)\n",
    "p2 = m2.predict(w8a_test_x)\n",
    "print(\"Accuracy:\" + \" \"+str(accuracy_score(w8a_test_y, p2)))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(np.array(range(n_feat)),m1.coef_, 'o-')\n",
    "plt.title('Coefficients values')\n",
    "plt.ylabel('I-th coeff')\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(np.array(range(num_it+1)), m1.l_f, 'r-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss-Function')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(np.array(range(n_feat)),m2.coef_, 'o-')\n",
    "plt.title('Coefficients values')\n",
    "plt.ylabel('I-th coeff')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(np.array(range(num_it+1)), m2.l_f, 'r-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss-Function')\n",
    "\n",
    "plt.subplots_adjust\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Mini Batch - not avaraging\"\n",
    "\"Dim 10\"\n",
    "dim_batch = 10\n",
    "m31 = LogisticRegression(lam, dim_batch, False, 'l', 1e-2, num_it)\n",
    "m31.fit(w8a_train_x, w8a_train_y)\n",
    "p31 = m31.predict(w8a_test_x)\n",
    "print(\"Accuracy:\" + \" \"+str(accuracy_score(w8a_test_y, p31)))\n",
    "\n",
    "\"Mini Batch - not avaraging\"\n",
    "\"Dim 20\"\n",
    "dim_batch = 20\n",
    "m32 = LogisticRegression(lam, dim_batch, False, 'l', 1e-2, num_it)\n",
    "m32.fit(w8a_train_x, w8a_train_y)\n",
    "p32 = m32.predict(w8a_test_x)\n",
    "print(\"Accuracy:\" + \" \"+str(accuracy_score(w8a_test_y, p32)))\n",
    "\n",
    "\"Mini Batch - not avaraging\"\n",
    "\"Dim 30\"\n",
    "dim_batch = 30\n",
    "m33 = LogisticRegression(lam, dim_batch, False, 'l', 1e-2, num_it)\n",
    "m33.fit(w8a_train_x, w8a_train_y)\n",
    "p33 = m33.predict(w8a_test_x)\n",
    "print(\"Accuracy:\" + \" \"+str(accuracy_score(w8a_test_y, p33)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(np.array(range(n_feat)),m31.coef_, 'o-')\n",
    "plt.title('Coefficients values')\n",
    "plt.ylabel('I-th coeff')\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(np.array(range(num_it+1)), m31.l_f, 'r-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss-Function')\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(np.array(range(n_feat)),m32.coef_, 'o-')\n",
    "plt.title('Coefficients values')\n",
    "plt.ylabel('I-th coeff')\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(np.array(range(num_it+1)), m32.l_f, 'r-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss-Function')\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(np.array(range(n_feat)),m33.coef_, 'o-')\n",
    "plt.title('Coefficients values')\n",
    "plt.ylabel('I-th coeff')\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(np.array(range(num_it+1)), m33.l_f, 'r-')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss-Function')\n",
    "\n",
    "plt.subplots_adjust\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
